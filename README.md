# 1470-final

**Report**: https://docs.google.com/document/d/1AImrBjX4PhIqd9G90TCWKgw94O3BsjLEAQncAU3PE0o/edit?usp=sharing

**Poster**：
![Alt text](DL_poster_final.jpg)


In this project, we aim to study and explore the Vision Permutator, an MLP-like architecture designed for image recognition. Unlike conventional architectures, it involves neither convolutional structures nor attention mechanisms. Instead, it solely relies on MLP-based modules combined with structured permutations across spatial and channel dimensions. Remarkably, such architectures can achieve performance comparable to, or even surpassing, that of models using CNNs or attention layers.
We plan to implement a slightly simplified version of the model described in “Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition” (arXiv:2106.12368v1 with GitHub), using TensorFlow. We will train and evaluate our model on one or more smaller datasets than those used in the original paper. The task is framed as an image classification problem.
Through this project, we hope to strengthen our ability to interpret and implement models proposed in academic literature, adapt them to new datasets, and potentially explore improvements or simplifications to the original design.

